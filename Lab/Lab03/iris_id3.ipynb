{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[6., 3., 4., 1., 1.],\n",
       "        [8., 3., 7., 2., 2.],\n",
       "        [6., 3., 4., 2., 1.],\n",
       "        [5., 4., 1., 0., 0.],\n",
       "        [8., 3., 7., 2., 2.],\n",
       "        [6., 3., 4., 1., 1.],\n",
       "        [5., 3., 1., 0., 0.],\n",
       "        [5., 4., 1., 0., 0.],\n",
       "        [5., 4., 2., 0., 0.],\n",
       "        [5., 2., 4., 1., 1.],\n",
       "        [6., 3., 5., 2., 2.],\n",
       "        [5., 3., 2., 0., 0.],\n",
       "        [5., 3., 2., 0., 0.],\n",
       "        [5., 3., 2., 0., 0.],\n",
       "        [6., 3., 4., 1., 1.],\n",
       "        [5., 3., 2., 0., 0.],\n",
       "        [6., 3., 4., 1., 1.],\n",
       "        [8., 4., 7., 2., 2.],\n",
       "        [5., 3., 1., 0., 0.],\n",
       "        [6., 3., 4., 1., 1.],\n",
       "        [6., 2., 5., 2., 2.],\n",
       "        [6., 4., 1., 0., 0.],\n",
       "        [6., 3., 5., 2., 2.],\n",
       "        [6., 3., 5., 2., 2.],\n",
       "        [6., 2., 4., 1., 1.],\n",
       "        [5., 3., 4., 2., 1.],\n",
       "        [6., 3., 5., 2., 2.],\n",
       "        [6., 2., 4., 1., 1.],\n",
       "        [5., 4., 2., 0., 0.],\n",
       "        [5., 2., 3., 1., 1.],\n",
       "        [6., 3., 5., 2., 2.],\n",
       "        [5., 3., 1., 0., 0.],\n",
       "        [5., 3., 1., 0., 0.],\n",
       "        [6., 2., 4., 1., 1.],\n",
       "        [7., 3., 5., 2., 1.],\n",
       "        [5., 3., 2., 0., 0.],\n",
       "        [6., 3., 5., 2., 2.],\n",
       "        [5., 3., 2., 0., 0.],\n",
       "        [5., 4., 2., 1., 0.],\n",
       "        [6., 3., 5., 2., 1.],\n",
       "        [5., 2., 3., 1., 1.],\n",
       "        [7., 3., 6., 2., 2.],\n",
       "        [6., 3., 5., 2., 1.],\n",
       "        [6., 3., 6., 1., 2.],\n",
       "        [8., 3., 6., 2., 2.],\n",
       "        [6., 2., 4., 1., 1.],\n",
       "        [4., 3., 1., 0., 0.],\n",
       "        [4., 3., 1., 0., 0.],\n",
       "        [6., 2., 5., 2., 2.],\n",
       "        [7., 3., 6., 2., 2.],\n",
       "        [5., 3., 2., 0., 0.],\n",
       "        [5., 4., 1., 0., 0.],\n",
       "        [4., 3., 1., 0., 0.],\n",
       "        [6., 2., 5., 2., 1.],\n",
       "        [6., 3., 6., 2., 2.],\n",
       "        [5., 3., 1., 0., 0.],\n",
       "        [7., 3., 6., 2., 2.],\n",
       "        [6., 3., 6., 2., 2.],\n",
       "        [5., 3., 1., 0., 0.],\n",
       "        [6., 3., 5., 1., 1.],\n",
       "        [6., 3., 5., 2., 1.],\n",
       "        [6., 3., 5., 2., 2.],\n",
       "        [7., 3., 5., 1., 1.],\n",
       "        [6., 3., 5., 2., 2.],\n",
       "        [5., 4., 2., 0., 0.],\n",
       "        [7., 3., 5., 2., 2.],\n",
       "        [6., 3., 4., 2., 1.],\n",
       "        [6., 3., 5., 2., 2.],\n",
       "        [6., 3., 4., 1., 1.],\n",
       "        [5., 3., 4., 1., 1.],\n",
       "        [6., 3., 5., 1., 1.],\n",
       "        [4., 2., 1., 0., 0.],\n",
       "        [7., 3., 5., 1., 1.],\n",
       "        [6., 3., 4., 1., 1.],\n",
       "        [5., 4., 2., 0., 0.],\n",
       "        [6., 3., 4., 1., 1.],\n",
       "        [7., 3., 6., 2., 2.],\n",
       "        [7., 3., 6., 2., 2.],\n",
       "        [5., 4., 2., 0., 0.],\n",
       "        [5., 2., 3., 1., 1.],\n",
       "        [7., 3., 6., 2., 2.],\n",
       "        [7., 3., 6., 2., 2.],\n",
       "        [5., 4., 1., 0., 0.],\n",
       "        [7., 3., 6., 2., 2.],\n",
       "        [5., 3., 1., 0., 0.],\n",
       "        [7., 3., 5., 2., 1.],\n",
       "        [7., 3., 6., 2., 2.],\n",
       "        [6., 3., 6., 2., 2.],\n",
       "        [6., 3., 4., 1., 1.],\n",
       "        [6., 3., 6., 2., 2.],\n",
       "        [6., 2., 4., 1., 1.],\n",
       "        [6., 3., 4., 1., 1.],\n",
       "        [6., 3., 5., 2., 2.],\n",
       "        [6., 3., 5., 2., 2.],\n",
       "        [5., 3., 2., 0., 0.],\n",
       "        [6., 3., 4., 1., 1.],\n",
       "        [5., 2., 4., 2., 2.],\n",
       "        [6., 4., 1., 0., 0.],\n",
       "        [6., 3., 4., 1., 1.],\n",
       "        [7., 3., 6., 2., 2.]]), array([[6., 3., 5., 1., 1.],\n",
       "        [6., 4., 2., 0., 0.],\n",
       "        [8., 3., 7., 2., 2.],\n",
       "        [6., 3., 4., 2., 1.],\n",
       "        [7., 3., 5., 1., 1.],\n",
       "        [5., 3., 2., 0., 0.],\n",
       "        [6., 3., 4., 1., 1.],\n",
       "        [7., 3., 5., 2., 2.],\n",
       "        [6., 2., 4., 2., 1.],\n",
       "        [6., 3., 4., 1., 1.],\n",
       "        [6., 3., 5., 2., 2.],\n",
       "        [5., 3., 1., 0., 0.],\n",
       "        [6., 4., 1., 0., 0.],\n",
       "        [5., 3., 2., 0., 0.],\n",
       "        [5., 4., 2., 0., 0.],\n",
       "        [6., 3., 5., 2., 1.],\n",
       "        [6., 3., 6., 2., 2.],\n",
       "        [6., 2., 4., 1., 1.],\n",
       "        [6., 3., 4., 1., 1.],\n",
       "        [6., 3., 6., 2., 2.],\n",
       "        [5., 3., 2., 0., 0.],\n",
       "        [6., 3., 5., 2., 2.],\n",
       "        [5., 3., 2., 0., 0.],\n",
       "        [6., 3., 6., 2., 2.],\n",
       "        [8., 4., 6., 2., 2.],\n",
       "        [7., 3., 5., 2., 2.],\n",
       "        [7., 2., 6., 2., 2.],\n",
       "        [7., 3., 6., 2., 2.],\n",
       "        [5., 3., 1., 0., 0.],\n",
       "        [5., 3., 2., 0., 0.],\n",
       "        [5., 4., 1., 0., 0.],\n",
       "        [6., 4., 2., 0., 0.],\n",
       "        [7., 3., 4., 1., 1.],\n",
       "        [5., 3., 2., 0., 0.],\n",
       "        [4., 3., 1., 0., 0.],\n",
       "        [6., 2., 5., 2., 2.],\n",
       "        [6., 3., 4., 2., 1.],\n",
       "        [5., 4., 2., 0., 0.],\n",
       "        [5., 4., 1., 0., 0.],\n",
       "        [5., 4., 2., 0., 0.],\n",
       "        [6., 3., 5., 2., 2.],\n",
       "        [6., 3., 4., 2., 1.],\n",
       "        [7., 3., 5., 2., 1.],\n",
       "        [5., 4., 1., 0., 0.],\n",
       "        [5., 4., 2., 0., 0.],\n",
       "        [6., 2., 4., 1., 1.],\n",
       "        [6., 3., 5., 2., 2.],\n",
       "        [6., 3., 6., 2., 2.],\n",
       "        [7., 3., 4., 1., 1.],\n",
       "        [7., 4., 6., 2., 2.]]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "def load_data():\n",
    "    iris = datasets.load_iris() #加载 iris 数据集\n",
    "    iris_feature = iris.data #特征数据\n",
    "    iris_target = iris.target #分类数据\n",
    "    labels = ['sepal length', 'sepal width', 'petal length', 'petal width'] # 定义标签\n",
    "    \n",
    "    # 划分训练集和测试集\n",
    "    feature_train, feature_test, target_train, target_test = train_test_split(iris_feature, iris_target, test_size=0.33, random_state=42)\n",
    "    \n",
    "    # 对训练集和测试集做简单处理，对连续float值转化为int处理，离散化连续变量\n",
    "    traindata = np.rint( np.column_stack((feature_train,target_train)) )\n",
    "    testdata = np.rint( np.column_stack((feature_test,target_test)) )\n",
    "    return traindata,testdata\n",
    "load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算信息熵\n",
    "def get_entropy(dataset):\n",
    "    \n",
    "    #创建所有类别列表：\n",
    "    label_tags = list(set(dataset[:, -1]))\n",
    "    label_length = len(dataset[:, -1])\n",
    "    tmp_entropy = 0\n",
    "    \n",
    "    # 计算信息熵\n",
    "    for label_tag in label_tags:\n",
    "        tmp = sum([1 for d in dataset if d[-1]==label_tag])\n",
    "        prob = tmp/label_length\n",
    "        tmp_entropy += (prob)*np.math.log(prob, 2)\n",
    "    entropy = -tmp_entropy\n",
    "    return entropy\n",
    "\n",
    "#计算某特征分类后的信息增益\n",
    "def get_rela_entropy(dataset, feature:int):\n",
    "    \n",
    "    # 生成某特征的取值范围\n",
    "    feature_tags = list(set(dataset[:, feature]))\n",
    "    sub_entropy = 0.0\n",
    "    \n",
    "    # 计算某个特征值的信息增益\n",
    "    for feature_tag in feature_tags:\n",
    "        sub_dataset = [d for d in dataset if d[feature]==feature_tag]\n",
    "        sub_dataset = np.array(sub_dataset)\n",
    "        \n",
    "        # 计算某特征值的信息熵\n",
    "        tmp_entropy = get_entropy(sub_dataset)\n",
    "        \n",
    "        # 计算分类后带权值的信息熵总和\n",
    "        sub_entropy += (len(sub_dataset)/len(dataset)) * tmp_entropy\n",
    "    \n",
    "    # 用父节点信息熵减去分类后子结点的权值信息熵总和，得出按某特征分类后的信息增益\n",
    "    rela_entropy = get_entropy(dataset) - sub_entropy\n",
    "    return rela_entropy\n",
    "\n",
    "# 选取当前结点集合的最佳分类特征\n",
    "def select_feature(dataset, features):\n",
    "    rela_entropys = list()\n",
    "    \n",
    "    # 将每个特征的信息增益存储到列表中\n",
    "    for feature in features:\n",
    "        feature:int\n",
    "        rela_entropy = get_rela_entropy(dataset, feature)\n",
    "        rela_entropys.append(rela_entropy)\n",
    "        \n",
    "    # 返回最大信息增益的特征\n",
    "    return features[rela_entropys.index(max(rela_entropys))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature': 2, 'children': {1.0: {'label': 0.0}, 2.0: {'label': 0.0}, 3.0: {'label': 1.0}, 4.0: {'feature': 0, 'children': {5.0: {'feature': 1, 'children': {2.0: {'feature': 3, 'children': {1.0: {'label': 1.0}, 2.0: {'label': 2.0}}}, 3.0: {'label': 1.0}}}, 6.0: {'label': 1.0}}}, 5.0: {'feature': 3, 'children': {1.0: {'label': 1.0}, 2.0: {'feature': 0, 'children': {6.0: {'feature': 1, 'children': {2.0: {'label': 2.0}, 3.0: {'label': 2.0}}}, 7.0: {'feature': 1, 'children': {3.0: {'label': 1.0}}}}}}}, 6.0: {'label': 2.0}, 7.0: {'label': 2.0}}}\n",
      "[1.0, 0.0, 2.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 1.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 2.0, 1.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 1.0, 0.0, 0.0, 0.0, 2.0, 1.0, 1.0, 0.0, 0.0, 1.0, 2.0, 2.0, 0.0, 2.0]\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "# 选取当前结点集合的最佳分类特征\n",
    "def select_feature(dataset, features):\n",
    "    rela_entropys = list()\n",
    "    \n",
    "    # 将每个特征的信息增益存储到列表中\n",
    "    for feature in features:\n",
    "        feature:int\n",
    "        rela_entropy = get_rela_entropy(dataset, feature)\n",
    "        rela_entropys.append(rela_entropy)\n",
    "        \n",
    "    # 返回最大信息增益的特征\n",
    "    return features[rela_entropys.index(max(rela_entropys))]\n",
    "\n",
    "# 返回当前结点的类别判定（判定为类中个数较多的）\n",
    "def major_label(labels):\n",
    "    \n",
    "    # 生成当前结点的类别集合\n",
    "    tags = list(set(labels))\n",
    "    tag_num = [sum([1 for i in labels if i==label]) for label in tags]\n",
    "    k = tag_num.index(max(tag_num))\n",
    "    \n",
    "    # 返回数量最多的类别\n",
    "    return tags[k]\n",
    "\n",
    "# 生成决策树，返回字典形式\n",
    "def build_tree(dataset, features) -> dict:\n",
    "    \n",
    "    \n",
    "    # 将当前结点的所有类别属性存入列表\n",
    "    labels = dataset[:, -1]\n",
    "    \n",
    "    # 第一种停止情况：当前结点属于同一类别则返回类别标记\n",
    "    if len(set(labels)) == 1:\n",
    "        return {'label': labels[0]}\n",
    "    \n",
    "    # 第二种情况：当前结点划分属性用完了\n",
    "    if not len(features):\n",
    "        return {'label': major_label(labels)}\n",
    "    \n",
    "    # 第三种情况：当前结点的所有属性取值相等\n",
    "    for feature in features:\n",
    "        f_tags = list(set(dataset[:, feature]))\n",
    "        if len(f_tags) == 1:\n",
    "            return {'label': major_label(labels)}\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # 选取根据信息增益选取最佳决策属性\n",
    "    best_feature = select_feature(dataset, features)\n",
    "    # print(\"best_feature:\",best_feature)\n",
    "    tree = {'feature': best_feature, 'children': {}}\n",
    "    \n",
    "    # 对该特征的每个取值进行决策划分\n",
    "    feature_tags = list(set(dataset[:, best_feature]))\n",
    "    for feature_tag in feature_tags:\n",
    "        sub_dataset = [d for d in dataset if d[best_feature]==feature_tag]\n",
    "        sub_dataset = np.array(sub_dataset)\n",
    "        \n",
    "        # 如果划分后的数据集为空，将其置为父结点类别\n",
    "        if len(sub_dataset) == 0:\n",
    "            tree['children'][feature_tag] = {'label_null': major_label(labels)}\n",
    "        else:\n",
    "            sub_features = [i for i in features if i != best_feature]\n",
    "            # print(\"sub_feature:\",sub_features)\n",
    "            tree['children'][feature_tag] = build_tree(sub_dataset, sub_features)\n",
    "    return tree\n",
    "\n",
    "def classify(tree:dict, sample):\n",
    "    for k, v in tree.items():\n",
    "        if k != 'feature':\n",
    "            return tree['label']\n",
    "        else:\n",
    "            return classify(tree['children'][sample[tree['feature']]], sample)\n",
    "\n",
    "        \n",
    "def classifier(tree:dict, features_data, default):\n",
    "    predict_vec = list()\n",
    "    for features_sample in features_data:\n",
    "        try:\n",
    "            predict = classify(tree, features_sample)\n",
    "        except KeyError:\n",
    "            predict = default\n",
    "        predict_vec.append(predict)\n",
    "    return predict_vec\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    train_data, test_data = load_data()\n",
    "    tree = build_tree(train_data, list(range(train_data.shape[1]-1)))\n",
    "    print(tree)\n",
    "    test_data_labels = test_data[:, -1]\n",
    "    test_data_features = test_data[:, :-1]\n",
    "    default = major_label(test_data_labels)\n",
    "    predict_vec = classifier(tree, test_data_features, default)\n",
    "    print(predict_vec)\n",
    "    accuracy = np.mean(np.array(predict_vec==test_data_labels))\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
